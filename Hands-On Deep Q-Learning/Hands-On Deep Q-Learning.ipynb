{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7612f8a-6ddc-40a1-a25c-e448d80e42ef",
   "metadata": {},
   "source": [
    "# Hands-on Deep Q-Learning\n",
    "As described [here](https://medium.com/towards-data-science/hands-on-deep-q-learning-9073040ce841)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b19310-f56d-4ceb-9ff7-c61a836e05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "n_episodes = 1000  # play 1000 games\n",
    "eps = 0.4  # exploration rate, probability of choosing random action\n",
    "eps_decay = 0.95  # eps gets multiplied by this number each epoch...\n",
    "min_eps = 0.1  # ...until this minimum eps is reached\n",
    "gamma = 0.95  # discount\n",
    "max_memory_size = 10000  # size of the replay memory\n",
    "batch_size = 16  # batch size of the neural network training\n",
    "min_length = 160  # minimum length of the replay memory for training, before it reached this length, no gradient updates happen\n",
    "memory_parts = [\n",
    "    \"state\",\n",
    "    \"action\",\n",
    "    \"next_state\",\n",
    "    \"reward\",\n",
    "    \"done\",\n",
    "]  # nice names for the part of replay memory, otherweise the names are 0-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be63a9e-04ec-4f87-b634-28c8d76a4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory = namedtuple(\"Memory\", memory_parts)  # a single entry of the memory replay\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_length=None):\n",
    "        self.max_length = max_length\n",
    "        self.memory = deque(maxlen=max_length)\n",
    "\n",
    "    def store(self, data):\n",
    "        self.memory.append(data)\n",
    "\n",
    "    def _sample(self, k):\n",
    "        return random.sample(self.memory, k)\n",
    "\n",
    "    def structured_sample(self, k):\n",
    "        batch = self._sample(k)\n",
    "        result = {}\n",
    "        for i, part in enumerate(memory_parts):\n",
    "            result[part] = np.array([row[i] for row in batch])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd1008-a94f-46cb-915f-ca878c5c6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(16, input_shape=(4,), activation=\"relu\"),  # state consists of 4 floats\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(2, activation=\"linear\"),  # 2 actions: go left or go right\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef067e6f-0c22-4d05-8ffd-b6124f4bb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "replay_memory = ReplayMemory(max_length=max_memory_size)\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):  # tqdm makes a nice proress bar\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if random.random() < eps:\n",
    "            action = env.action_space.sample()  # random action\n",
    "        else:\n",
    "            action = model.predict(state[np.newaxis, :], verbose=False).argmax()  # best action according to the model\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        memory = Memory(state, action, next_state, reward, done)\n",
    "        replay_memory.store(memory)\n",
    "\n",
    "        if len(replay_memory) >= min_length:\n",
    "            batch = replay_memory.structured_sample(batch_size)  # get samples from the replay memory\n",
    "\n",
    "            target_batch = batch[\"reward\"] + gamma * model.predict(batch[\"next_state\"], verbose=False).max(axis=1) * (\n",
    "                1 - batch[\"done\"]\n",
    "            )  # R(s, a) + γ·maxₐ N(s') if not a terminal state, otherwise R(s, a)\n",
    "            targets = model.predict(batch[\"state\"], verbose=False)\n",
    "            targets[\n",
    "                range(batch_size), batch[\"action\"]\n",
    "            ] = target_batch  # set the target for the action that was done and leave the outputs of other 3 actions as they are\n",
    "\n",
    "            model.fit(batch[\"state\"], targets, verbose=False, batch_size=batch_size)  # train for one epoch\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    eps = max(min_eps, eps * eps_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d558a069-825c-4882-a1cf-8835aed434c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"PATH/TO/MODEL/230\")\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done and total_reward < 500:  # force end the game after 500 time steps because the model is too good!\n",
    "    env.render()\n",
    "    action = model.predict(state[np.newaxis, :], verbose=False).argmax(axis=1)[0]\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc49de-1609-4be0-8cc9-dabdf38e547f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "py310",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
